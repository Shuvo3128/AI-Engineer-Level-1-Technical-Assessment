# -*- coding: utf-8 -*-
"""RAG System .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fEPI-gmAE0gKBWToOH6Ly1rGPYy7JNWS
"""

!pip install -q langchain faiss-cpu sentence-transformers openai
!pip install -q unstructured pdfminer.six
!pip install -q pymupdf
!pip install -U langchain-community
!pip install pytesseract pillow
!sudo apt install tesseract-ocr tesseract-ocr-ben

from google.colab import files
uploaded = files.upload()  # Upload your "HSC26-Bangla1st-Paper.pdf"

import fitz
import pytesseract
from PIL import Image
import re
import unicodedata
from rapidfuzz import fuzz, process

#1.Extract OCR Text from PDF Pages
def extract_ocr_text(pdf_path, start_page, end_page):
    doc = fitz.open(pdf_path)
    extracted_text = []
    for i in range(start_page - 1, end_page):
        pix = doc[i].get_pixmap(dpi=300)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        text = pytesseract.image_to_string(img, lang="ben")
        extracted_text.append(text)
    return "\n".join(extracted_text)

#2.Normalize Unicode
def normalize_unicode(text):
    return unicodedata.normalize("NFKC", text)

def fix_common_errors(text):
    corrections = {
        '‡¶Ü‡¶≤‡¶æ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ö': '‡¶Ö‡¶®‡¶≤‡¶æ‡¶á‡¶® ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ö',
        '‡¶Ü‡¶≤‡¶æ': '‡¶Ö‡¶®‡¶≤‡¶æ‡¶á‡¶®',
        '‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶¨': '‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®',
        ' ‡ßã‡ßã‡¶≤‡¶ï': '‡¶≤‡ßã‡¶ï',
        '‡¶ø ‡¶æ‡¶ø ‡¶ï‡¶æ‡¶ø‡¶£': '‡¶ï‡¶ø ‡¶ï‡¶æ‡¶∞‡¶£‡ßá',
        '‡¶∞‡¶¨‡ßç‡¶ö‡¶ï‡ßç‡¶∑‡¶£‡¶§‡¶æ': '‡¶¨‡ßÅ‡¶¶‡ßç‡¶ß‡¶ø‡¶Æ‡¶§‡ßç‡¶§‡¶æ',
        '‡¶ï‡ßÇ‡¶ü ‡¶¨‡ßÅ‡¶∞‡¶ø': '‡¶ï‡ßÇ‡¶ü‡¶¨‡ßÅ‡¶¶‡ßç‡¶ß‡¶ø',
        '‡¶â‡¶ï‡¶¶‡¶Ø‡¶æ‡¶ó': '‡¶â‡¶¶‡ßç‡¶Ø‡ßã‡¶ó',
        '‡¶∞‡¶®‡¶ï‡¶≤‡¶ì': '‡¶®‡¶ø‡¶≤‡ßá‡¶ì',
        '‡¶∞‡¶®‡¶ï': '‡¶®‡¶æ',
        '‡¶¨‡¶æ‡¶º‡ßã‡¶¨‡¶æ‡¶∞‡¶º‡ßá': '‡¶¨‡¶æ‡¶ß‡¶æ‡¶¨‡ßç‡¶Ø‡¶∞‡ßç‡¶•',
        '‡¶ï‡¶¨‡¶æ‡¶ø ‡¶ï‡¶æ‡¶ø‡¶ï‡¶£': '‡¶ï‡ßã‡¶® ‡¶ï‡¶æ‡¶∞‡¶£‡ßá',
        '‡¶∞‡¶™‡¶§‡¶æ': '‡¶π‡¶Ø‡¶º‡¶§‡ßã',
        '‡¶§‡¶æ‡¶ø': '‡¶§‡¶¨‡ßá',
        '‡¶π‡¶ï‡¶≤‡¶ì': '‡¶π‡¶≤‡ßá‡¶ì',
        '‡¶ï‡¶§‡¶¨‡¶æ': '‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ',
        '‡¶ï‡¶®‡¶Ø‡¶æ‡¶ø': '‡¶ï‡¶®‡ßç‡¶Ø‡¶æ',
        '‡¶∞‡¶∏‡¶¨‡¶æ‡¶®‡ßç‡¶§': '‡¶®‡¶ø‡¶ú‡ßá‡¶∞',
        '‡¶∞‡¶ø‡¶ï‡¶≤‡¶®': '‡¶≤‡¶æ‡¶≤‡¶®',
        '‡¶™‡¶∞‡¶ø‡¶¨‡¶æ‡¶ï‡¶ø‡¶ø': '‡¶™‡¶∞‡¶ø‡¶¨‡¶æ‡¶∞‡ßá‡¶∞',
        '‡¶∞‡¶ø‡¶ú‡¶ï‡ßç‡¶∑‡¶§': '‡¶Æ‡ßá‡¶ß‡¶æ‡¶¨‡ßÄ',
        '‡¶â‡¶ø‡¶ø': '‡¶â‡¶§‡ßç‡¶§‡¶∞',
        '‡¶™‡¶ï‡¶º‡ßá': '‡¶™‡¶°‡¶º‡ßá',
        '‡¶™‡ßç‡¶∞‡¶ï‡ßá‡¶ø': '‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®‡ßá‡¶∞',
        '‡¶¶‡ßá‡¶ø‡¶§‡¶æ‡¶ø': '‡¶¶‡ßç‡¶¨‡¶ø‡¶§‡ßÄ‡¶Ø‡¶º',
        '‡¶è‡¶≤‡¶ú‡¶®‡ßç‡¶ü': '‡¶â‡¶™‡¶æ‡¶¶‡¶æ‡¶®',
        '‡¶™‡ßç‡¶∞‡¶∞‡ßç‡¶§‡¶™‡¶ú‡¶ø': '‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶Ø‡ßã‡¶ó‡¶ø‡¶§‡¶æ',
        '‡¶¨‡ßç‡ßÅ‡¶∞‡ßç‡¶ø': '‡¶¨‡ßÅ‡¶¶‡ßç‡¶ß‡¶ø',
        '‡ßç‡ßç': '‡ßç',
        '\xa0': ' ',
        '  ': ' '
    }
    for wrong, correct in corrections.items():
        text = text.replace(wrong, correct)
    return text

expected_terms = [
    "‡¶Ö‡¶®‡¶≤‡¶æ‡¶á‡¶®", "‡¶¨‡ßç‡¶Ø‡¶æ‡¶ö", "‡¶¨‡ßã‡¶∞‡ßç‡¶°", "‡¶™‡¶∞‡ßÄ‡¶ï‡ßç‡¶∑‡¶æ", "‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®", "‡¶â‡¶§‡ßç‡¶§‡¶∞", "‡¶¨‡ßÅ‡¶¶‡ßç‡¶ß‡¶ø‡¶Æ‡¶§‡ßç‡¶§‡¶æ",
    "‡¶â‡¶¶‡ßç‡¶Ø‡ßã‡¶ó", "‡¶ï‡¶®‡ßç‡¶Ø‡¶æ", "‡¶ï‡¶≤‡ßç‡¶Ø‡¶æ‡¶£‡ßÄ", "‡¶Ö‡¶™‡¶∞‡¶ø‡¶ö‡¶ø‡¶§‡¶æ", "‡¶¨‡¶ø‡¶®‡ßÅ‡¶¶‡¶æ", "‡¶π‡ßÄ‡¶®‡¶Æ‡¶®‡ßç‡¶Ø‡¶§‡¶æ"
]

def fuzzy_correct(text, vocab=expected_terms, threshold=85):
    words = text.split()
    corrected_words = []
    for word in words:
        match, score, _ = process.extractOne(word, vocab, scorer=fuzz.ratio)
        if score >= threshold:
            corrected_words.append(match)
        else:
            corrected_words.append(word)
    return ' '.join(corrected_words)

def fix_mcq_question_header(text):
    text = re.sub(r'‡¶Ü‡¶≤‡¶æ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ö ‡¶¨‡ßã‡¶∞‡ßç‡¶° ‡¶™‡¶∞‡ßÄ‡¶ï‡ßç‡¶∑‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶¨', '‡¶Ö‡¶®‡¶≤‡¶æ‡¶á‡¶® ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ö ‡¶¨‡ßã‡¶∞‡ßç‡¶° ‡¶™‡¶∞‡ßÄ‡¶ï‡ßç‡¶∑‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®', text)
    return text

def remove_markup(text):
    return re.sub(r'<[^>]+>', '', text)

#7. Remove Punctuation (preserve Bangla ones)
def remove_punctuations(text):
    bangla_keep = "‡•§!?‡¶É"
    return re.sub(rf'[^\u0980-\u09FF\s{bangla_keep}]', ' ', text)

# 8. Advanced Text Cleaning ===
def advanced_cleaning(text):
    text = re.sub(r'(.)\1{2,}', r'\1\1', text)
    text = re.sub(r'[‚Äú‚Äù‚Äò‚Äô"\'`]+', '', text)
    text = re.sub(r'[\u200c\u200d]+', '', text)
    text = re.sub(r'\s+', ' ', text)
    return text

#9. Remove Table-like Lines
def remove_table_lines(text):
    lines = text.split('\n')
    clean_lines = []
    for line in lines:
        if sum(c.isdigit() for c in line) > 10:
            continue
        if line.count('|') > 2 or line.count('.') > 5:
            continue
        if re.fullmatch(r'([^\u0980-\u09FF\s]{1,5}\s*)+', line.strip()):
            continue
        clean_lines.append(line)
    return '\n'.join(clean_lines)

#9.Format Structure
def format_structure(text):
    text = re.sub(r'([‡¶ï-‡¶ò])[\)\.]', r'\n\1)', text)
    text = re.sub(r'([‡•§!?])', r'\1\n', text)
    text = re.sub(r'\n+', '\n', text)
    return text

# 10.Line Filtering ===
def filter_lines(text):
    lines = text.split('\n')
    return '\n'.join([line.strip() for line in lines if 10 < len(line.strip()) < 200])

#11.Save to File ===
def save_output(text, filename):
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(text)

#MAIN PIPELINE
if __name__ == "__main__":
    pdf_path = "HSC26-Bangla1st-Paper.pdf"
    start_page, end_page = 6, 40

    raw = extract_ocr_text(pdf_path, start_page, end_page)
    norm = normalize_unicode(raw)
    fixed = fix_common_errors(norm)
    fixed = fix_mcq_question_header(fixed)  # Specific fixes
    fuzzy_fixed = fuzzy_correct(fixed)      # Fuzzy word correction
    markup_free = remove_markup(fuzzy_fixed)
    no_punc = remove_punctuations(markup_free)
    clean = advanced_cleaning(no_punc)
    table_removed = remove_table_lines(clean)  # ‚úÖ Remove table-like content
    structured = format_structure(table_removed)
    structured = format_structure(clean)
    final = filter_lines(structured)

    # Show sample output
    print("\nüìù Sample Cleaned Output (First 20 Lines):\n")
    for line in final.split('\n')[:20]:
        print(line)

    # Save final output
    save_output(final, "cleaned_hsc26_p6_40_advanced.txt")
    print("‚úÖ Cleaned file saved to 'cleaned_hsc26_p6_40_advanced.txt'")

!apt install tesseract-ocr
!apt install tesseract-ocr-ben
!pip install pdf2image pytesseract
!apt-get install -y poppler-utils tesseract-ocr tesseract-ocr-ben

from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.document_loaders import TextLoader
import os

loader = TextLoader("cleaned_hsc26_p6_40_advanced.txt", encoding="utf-8")
documents = loader.load()

# Clean metadata from docs
for doc in documents:
    doc.metadata = {}

#Optimized chunking: smaller size, more overlap
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=300,
    chunk_overlap=80,
    separators=["\n", "‡•§", ".", "?", "!"]
)
chunks = text_splitter.split_documents(documents)

embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/LaBSE")

#Vectorize and store
vectorstore = FAISS.from_documents(chunks, embedding_model)
vectorstore.save_local("bangla_knowledge_base")

#Load for retrieval
db = FAISS.load_local("bangla_knowledge_base", embedding_model, allow_dangerous_deserialization=True)
retriever = db.as_retriever(search_type="similarity", search_kwargs={"k": 5})

# Matching function with partial + case-insensitive match
def test_chunk_retrieval(question, expected, retriever):
    docs = retriever.get_relevant_documents(question)
    print(f"\nüîç Question: {question}")
    print(f"‚úÖ Expected Answer: {expected}")

    matched = False
    expected_lower = expected.strip().lower()
    for i, doc in enumerate(docs):
        chunk_text = doc.page_content.strip().lower()
        print(f"\nüìÑ Chunk {i+1}:\n{doc.page_content[:400]}")
        if expected_lower in chunk_text:
            matched = True

    print(f"\nüéØ Chunk Match Found: {'‚úÖ Yes' if matched else '‚ùå No'}")
    return matched

TEST_CASES = {
    "‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ‡ßá‡¶∞ ‡¶≠‡¶æ‡¶∑‡¶æ‡¶Ø‡¶º ‡¶∏‡ßÅ‡¶™‡ßÅ‡¶∞‡ßÅ‡¶∑ ‡¶ï‡¶æ‡¶ï‡ßá ‡¶¨‡¶≤‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá?": "‡¶∂‡ßÅ‡¶Æ‡ßç‡¶≠‡ßÅ‡¶®‡¶æ‡¶•",
    "‡¶ï‡¶æ‡¶ï‡ßá ‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ‡ßá‡¶∞ ‡¶≠‡¶æ‡¶ó‡ßç‡¶Ø ‡¶¶‡ßá‡¶¨‡¶§‡¶æ ‡¶¨‡¶≤‡ßá ‡¶â‡¶≤‡ßç‡¶≤‡ßá‡¶ñ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá?": "‡¶Æ‡¶æ‡¶Æ‡¶æ‡¶ï‡ßá",
    "‡¶¨‡¶ø‡¶Ø‡¶º‡ßá‡¶∞ ‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶ï‡¶≤‡ßç‡¶Ø‡¶æ‡¶£‡ßÄ‡¶∞ ‡¶™‡ßç‡¶∞‡¶ï‡ßÉ‡¶§ ‡¶¨‡¶Ø‡¶º‡¶∏ ‡¶ï‡¶§ ‡¶õ‡¶ø‡¶≤?": "‡ßß‡ß´ ‡¶¨‡¶õ‡¶∞",
}

#Evaluate
correct = 0
for q, expected in TEST_CASES.items():
    if test_chunk_retrieval(q, expected, retriever):
        correct += 1

accuracy = correct / len(TEST_CASES)
print(f"\nüî¢ Final Chunk Retrieval Accuracy: {accuracy:.2%}")

!pip install -q langchain faiss-cpu sentence-transformers

from langchain.document_loaders import TextLoader
loader = TextLoader("cleaned_hsc26_p6_40_advanced.txt", encoding="utf-8")
documents = loader.load()
for doc in documents:
    doc.metadata = {}

from langchain.text_splitter import RecursiveCharacterTextSplitter


text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=300,
    chunk_overlap=80,
    separators=["\n", "‡•§", ".", "?", "!"]
)

chunks = text_splitter.split_documents(documents)

print(f"‚úÖ Total Chunks Created: {len(chunks)}")

from langchain.embeddings import HuggingFaceEmbeddings
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/LaBSE")

from langchain.vectorstores import FAISS

vectorstore = FAISS.from_documents(chunks, embedding_model)
vectorstore.save_local("bangla_knowledge_base")

print("‚úÖ Vectorstore created and saved successfully.")

db = FAISS.load_local("bangla_knowledge_base", embedding_model, allow_dangerous_deserialization=True)
retriever = db.as_retriever(search_type="similarity", search_kwargs={"k": 5})
query = "‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ‡ßá‡¶∞ ‡¶≠‡¶æ‡¶∑‡¶æ‡¶Ø‡¶º ‡¶∏‡ßÅ‡¶™‡ßÅ‡¶∞‡ßÅ‡¶∑ ‡¶ï‡¶æ‡¶ï‡ßá ‡¶¨‡¶≤‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá?"
docs = retriever.get_relevant_documents(query)

for i, doc in enumerate(docs):
    print(f"\nüìÑ Chunk {i+1}:\n{doc.page_content[:300]}")

!pip install -U sentence-transformers faiss-cpu matplotlib scikit-learn
!pip uninstall -y scikit-learn
!pip install scikit-learn==1.3.2

from sentence_transformers import SentenceTransformer
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import faiss
import numpy as np

chunks = [
    "‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ‡ßá‡¶∞ ‡¶≠‡¶æ‡¶∑‡¶æ‡¶Ø‡¶º ‡¶∏‡ßÅ‡¶™‡ßÅ‡¶∞‡ßÅ‡¶∑ ‡¶∂‡¶Æ‡ßç‡¶≠‡ßÅ‡¶®‡¶æ‡¶•‡•§",
    "‡¶ï‡¶≤‡ßç‡¶Ø‡¶æ‡¶£‡ßÄ‡¶∞ ‡¶™‡ßç‡¶∞‡¶ï‡ßÉ‡¶§ ‡¶¨‡ßü‡¶∏ ‡¶õ‡¶ø‡¶≤ ‡ßß‡ß´ ‡¶¨‡¶õ‡¶∞‡•§",
    "‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ ‡¶Æ‡¶æ‡¶Æ‡¶æ‡¶ï‡ßá ‡¶§‡¶æ‡¶∞ ‡¶≠‡¶æ‡¶ó‡ßç‡¶Ø‡¶¶‡ßá‡¶¨‡¶§‡¶æ ‡¶¨‡¶≤‡ßá ‡¶Æ‡¶®‡ßá ‡¶ï‡¶∞‡ßá‡•§",
    "‡¶Æ‡¶æ‡¶Æ‡¶æ ‡¶õ‡¶ø‡¶≤‡ßá‡¶® ‡¶Ö‡¶§‡ßç‡¶Ø‡¶®‡ßç‡¶§ ‡¶Ø‡¶§‡ßç‡¶®‡¶¨‡¶æ‡¶® ‡¶ì ‡¶¶‡¶æ‡ßü‡¶ø‡¶§‡ßç‡¶¨‡¶∂‡ßÄ‡¶≤‡•§",
    "‡¶ï‡¶¨‡¶ø‡¶§‡¶æ‡¶ü‡¶ø ‡¶¨‡¶æ‡¶∏‡ßç‡¶§‡¶¨ ‡¶ú‡ßÄ‡¶¨‡¶® ‡¶•‡ßá‡¶ï‡ßá ‡¶Ö‡¶®‡ßÅ‡¶™‡ßç‡¶∞‡¶æ‡¶£‡¶ø‡¶§‡•§"
]


model = SentenceTransformer('sentence-transformers/LaBSE')

embeddings = model.encode(chunks)
pca = PCA(n_components=2)
reduced = pca.fit_transform(embeddings)

plt.figure(figsize=(10, 7))
for i, text in enumerate(chunks):
    x, y = reduced[i]
    plt.scatter(x, y, color='blue')
    plt.text(x + 0.01, y + 0.01, f"{i+1}", fontsize=9)
plt.title("üìå Document Chunks - PCA Projection")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.grid(True)
plt.show()

!pip install langchain openai langchain-community
!pip install -U langchain langchain-community langchain-openai

from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain.chat_models import ChatOpenAI

embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/LaBSE")

db = FAISS.load_local("bangla_knowledge_base", embedding_model, allow_dangerous_deserialization=True)
retriever = db.as_retriever(search_kwargs={"k": 10})  # Use top-10 most relevant chunks

llm = ChatOpenAI(temperature=0, openai_api_key="your-openai-api-key")  # Replace with your key

memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

rag_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    memory=memory,
    verbose=True  # Enable to see what's happening
)

while True:
    user_input = input("üßë‚Äçüí¨ You: ")
    if user_input.lower() in ["exit", "quit"]:
        break
    response = rag_chain({"question": user_input})
    print(f"ü§ñ RAG Answer: {response['answer']}")

import os
os.environ["OPENAI_API_KEY"] = "sk-proj-"

from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain_openai import ChatOpenAI
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings

# ‚úÖ Load FAISS DB (Long-Term Memory)
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/LaBSE")
vectorstore = FAISS.load_local("bangla_knowledge_base", embedding_model, allow_dangerous_deserialization=True)
retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 5})

# ‚úÖ Load OpenAI model or any Chat LLM
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.2)

# ‚úÖ Short-Term Memory Buffer
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

rag_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    memory=memory,
    return_source_documents=True
)

chat_history = []

questions = [
    "‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ‡ßá‡¶∞ ‡¶≠‡¶æ‡¶∑‡¶æ‡¶Ø‡¶º ‡¶∏‡ßÅ‡¶™‡ßÅ‡¶∞‡ßÅ‡¶∑ ‡¶ï‡¶æ‡¶ï‡ßá ‡¶¨‡¶≤‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá?",
    "‡¶ï‡¶æ‡¶ï‡ßá ‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ‡ßá‡¶∞ ‡¶≠‡¶æ‡¶ó‡ßç‡¶Ø ‡¶¶‡ßá‡¶¨‡¶§‡¶æ ‡¶¨‡¶≤‡ßá ‡¶â‡¶≤‡ßç‡¶≤‡ßá‡¶ñ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá?",
    "‡¶¨‡¶ø‡¶Ø‡¶º‡ßá‡¶∞ ‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶ï‡¶≤‡ßç‡¶Ø‡¶æ‡¶£‡ßÄ‡¶∞ ‡¶™‡ßç‡¶∞‡¶ï‡ßÉ‡¶§ ‡¶¨‡¶Ø‡¶º‡¶∏ ‡¶ï‡¶§ ‡¶õ‡¶ø‡¶≤?"
]

# Loop through questions to simulate a session
for i, question in enumerate(questions, 1):
    response = rag_chain({"question": question, "chat_history": chat_history})
    print(f"\nüîπ User Question {i}: {question}")
    print(f"‚úÖ Answer: {response['answer']}")

    # Add to short-term memory
    chat_history.append((question, response["answer"]))